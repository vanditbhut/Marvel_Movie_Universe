{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sgJiE4PxC3c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "import boto3\n",
        "from bs4 import BeautifulSoup\n",
        "from io import StringIO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your OMDB API Key and S3 bucket name directly in the code\n",
        "OMDB_API_KEY = \"124b44b3\"  # Replace with your actual OMDB API key\n",
        "S3_BUCKET_NAME = \"van-movie-data-bucket\"  # Replace with your actual S3 bucket name"
      ],
      "metadata": {
        "id": "5TygI1ImxqZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Scrape Marvel Movie Titles\n",
        "\n",
        "def scrape_marvel_movies():\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_Marvel_Cinematic_Universe_films\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    tables = soup.find_all('table', class_='wikitable')\n",
        "\n",
        "    all_data = []\n",
        "    for i, table in enumerate(tables[:7]):  # Phases 1-6 tables\n",
        "        headers = [header.text.strip() for header in table.find_all('th')]\n",
        "        rows = table.find_all('tr')\n",
        "        data = []\n",
        "        columns = ['film', 'release_date', 'director', 'writer', 'producer', \"status\"]\n",
        "\n",
        "        for row in rows:\n",
        "            cols = row.find_all(['td', 'th'])\n",
        "            cols = [ele.text.strip() for ele in cols]\n",
        "            if len(cols) < len(columns):\n",
        "                cols.extend([None] * (len(columns) - len(cols)))\n",
        "            elif len(cols) > len(columns):\n",
        "                cols = cols[:len(columns)]\n",
        "            data.append(cols)\n",
        "\n",
        "        df = pd.DataFrame(data[1:], columns=columns)\n",
        "        df['phase'] = f\"Phase {i+1}\"\n",
        "        all_data.append(df)\n",
        "\n",
        "    return pd.concat(all_data, ignore_index=True)"
      ],
      "metadata": {
        "id": "7BCF-DA5xlfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Clean Movie Data\n",
        "\n",
        "def clean_movie_data(movies_df):\n",
        "    movies_df['producer'] = movies_df['producer'].fillna(method='ffill')\n",
        "    movies_df['status'] = movies_df['status'].fillna(method='ffill')\n",
        "\n",
        "    def remove_references(text):\n",
        "        return re.sub(r'\\s*\\[\\s*\\d+\\s*\\]', '', text)\n",
        "\n",
        "    movies_df_cleaned = movies_df.applymap(lambda cell: remove_references(cell) if isinstance(cell, str) else cell)\n",
        "\n",
        "    movies_df_cleaned['release_date'] = pd.to_datetime(\n",
        "        movies_df_cleaned['release_date'].str.extract(r'\\((.*?)\\)')[0], errors='coerce'\n",
        "    )\n",
        "    movies_df_cleaned['release_date'] = movies_df_cleaned['release_date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "    return movies_df_cleaned"
      ],
      "metadata": {
        "id": "BV7Rs3Wgxg6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3 : Scrape Character Data\n",
        "\n",
        "def fetch_omdb_data(film_name):\n",
        "    url = f'http://www.omdbapi.com/?t={film_name}&apikey={OMDB_API_KEY}'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return {\"Title\": film_name, \"Error\": \"Data not found\"}"
      ],
      "metadata": {
        "id": "8u18i79txhC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Scrape Characters Data\n",
        "\n",
        "def scrape_characters_data():\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_Marvel_Cinematic_Universe_films\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    table = None\n",
        "\n",
        "    for tbl in soup.find_all('table', class_='wikitable'):\n",
        "        caption = tbl.find('caption')\n",
        "        if caption and \"Recurring cast and characters of Marvel Cinematic Universe films\" in caption.get_text():\n",
        "            table = tbl\n",
        "            break\n",
        "\n",
        "    if table:\n",
        "        headers = [th.get_text(strip=True) for th in table.find_all('tr')[0].find_all('th')]\n",
        "        rows = []\n",
        "\n",
        "        for tr in table.find_all('tr')[1:]:\n",
        "            row = []\n",
        "            for td in tr.find_all(['th', 'td']):\n",
        "                colspan = td.get('colspan')\n",
        "                if colspan:\n",
        "                    colspan = int(colspan)\n",
        "                    row.extend([td.get_text(separator=\" \", strip=True)] * colspan)\n",
        "                else:\n",
        "                    row.append(td.get_text(separator=\" \", strip=True))\n",
        "\n",
        "            while len(row) < len(headers):\n",
        "                row.append(None)\n",
        "\n",
        "            if len(row) > len(headers):\n",
        "                row = row[:len(headers)]  # Truncate to match header length\n",
        "\n",
        "            rows.append(row)\n",
        "\n",
        "        return pd.DataFrame(rows, columns=headers)\n",
        "    else:\n",
        "        print(\"Table with specified caption not found.\")\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "Pu596tdMxGf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Fetch Movie Data from OMDB API\n",
        "\n",
        "def fetch_omdb_data(film_name):\n",
        "    url = f'http://www.omdbapi.com/?t={film_name}&apikey={OMDB_API_KEY}'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return {\"Title\": film_name, \"Error\": \"Data not found\"}"
      ],
      "metadata": {
        "id": "p8w4Q4DCxGjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Upload DataFrames to S3 with specific folder paths\n",
        "def upload_to_s3(df, file_name):\n",
        "    s3 = boto3.client('s3')\n",
        "    csv_buffer = StringIO()\n",
        "    df.to_csv(csv_buffer, index=False)\n",
        "    s3.put_object(Bucket=S3_BUCKET_NAME, Key=f\"{file_name}\", Body=csv_buffer.getvalue())"
      ],
      "metadata": {
        "id": "jTQVnCrKxGmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lambda handler function to call each step\n",
        "def lambda_handler(event, context):\n",
        "    # Step 1: Scrape movie titles from Wikipedia\n",
        "    movies_df = scrape_marvel_movies()\n",
        "\n",
        "    # Step 2: Clean the scraped movie data\n",
        "    movies_df_cleaned = clean_movie_data(movies_df)\n",
        "\n",
        "    # Step 3: Fetch additional movie data from OMDB API\n",
        "    omdb_results = []\n",
        "    for film in movies_df_cleaned['film']:\n",
        "        omdb_data = fetch_omdb_data(film)\n",
        "        omdb_results.append(omdb_data)\n",
        "\n",
        "    # Convert OMDB results to DataFrame\n",
        "    omdb_df = pd.DataFrame(omdb_results)\n",
        "\n",
        "    # Step 4: Scrape characters data\n",
        "    characters_df = scrape_characters_data()\n",
        "\n",
        "    # Upload DataFrames to S3 with specified folder paths\n",
        "    upload_to_s3(movies_df_cleaned, 'movies.csv')\n",
        "    upload_to_s3(omdb_df, 'omdb.csv')\n",
        "    upload_to_s3(characters_df, 'characters.csv')\n",
        "\n",
        "    return {\n",
        "        'statusCode': 200,\n",
        "        'body': 'Data uploaded successfully!'\n",
        "    }"
      ],
      "metadata": {
        "id": "8yfpmNLDxGp-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}